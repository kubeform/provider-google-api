apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/name: google.kubeform.com
    app.kubernetes.io/part-of: kubeform.com
  name: jobs.dataproc.google.kubeform.com
spec:
  group: dataproc.google.kubeform.com
  names:
    kind: Job
    listKind: JobList
    plural: jobs
    singular: job
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .status.phase
      name: Phase
      type: string
    name: v1alpha1
    schema:
      openAPIV3Schema:
        properties:
          apiVersion:
            description: 'APIVersion defines the versioned schema of this representation
              of an object. Servers should convert recognized schemas to the latest
              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
            type: string
          kind:
            description: 'Kind is a string value representing the REST resource this
              object represents. Servers may infer this from the endpoint the client
              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
            type: string
          metadata:
            type: object
          spec:
            properties:
              kubeformOutput:
                properties:
                  driverControlsFilesURI:
                    description: Output-only. If present, the location of miscellaneous
                      control files which may be used as part of job setup and handling.
                      If not present, control files may be placed in the same location
                      as driver_output_uri.
                    type: string
                  driverOutputResourceURI:
                    description: Output-only. A URI pointing to the location of the
                      stdout of the job's driver program
                    type: string
                  forceDelete:
                    description: By default, you can only delete inactive jobs within
                      Dataproc. Setting this to true, and calling destroy, will ensure
                      that the job is first cancelled before issuing the delete.
                    type: boolean
                  hadoopConfig:
                    description: The config of Hadoop job
                    properties:
                      archiveUris:
                        description: HCFS URIs of archives to be extracted in the
                          working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
                        items:
                          type: string
                        type: array
                      args:
                        description: The arguments to pass to the driver.
                        items:
                          type: string
                        type: array
                      fileUris:
                        description: HCFS URIs of files to be copied to the working
                          directory of Spark drivers and distributed tasks. Useful
                          for naively parallel tasks.
                        items:
                          type: string
                        type: array
                      jarFileUris:
                        description: HCFS URIs of jar files to add to the CLASSPATHs
                          of the Spark driver and tasks.
                        items:
                          type: string
                        type: array
                      loggingConfig:
                        description: The runtime logging config of the job
                        properties:
                          driverLogLevels:
                            additionalProperties:
                              type: string
                            description: 'Optional. The per-package log levels for
                              the driver. This may include ''root'' package name to
                              configure rootLogger. Examples: ''com.google = FATAL'',
                              ''root = INFO'', ''org.apache = DEBUG''.'
                            type: object
                        required:
                        - driverLogLevels
                        type: object
                      mainClass:
                        description: The class containing the main method of the driver.
                          Must be in a provided jar or jar that is already on the
                          classpath. Conflicts with main_jar_file_uri
                        type: string
                      mainJarFileURI:
                        description: The HCFS URI of jar file containing the driver
                          jar. Conflicts with main_class
                        type: string
                      properties:
                        additionalProperties:
                          type: string
                        description: A mapping of property names to values, used to
                          configure Spark. Properties that conflict with values set
                          by the Cloud Dataproc API may be overwritten. Can include
                          properties set in /etc/spark/conf/spark-defaults.conf and
                          classes in user code.
                        type: object
                    type: object
                  hiveConfig:
                    description: The config of hive job
                    properties:
                      continueOnFailure:
                        description: Whether to continue executing queries if a query
                          fails. The default value is false. Setting to true can be
                          useful when executing independent parallel queries. Defaults
                          to false.
                        type: boolean
                      jarFileUris:
                        description: HCFS URIs of jar files to add to the CLASSPATH
                          of the Hive server and Hadoop MapReduce (MR) tasks. Can
                          contain Hive SerDes and UDFs.
                        items:
                          type: string
                        type: array
                      properties:
                        additionalProperties:
                          type: string
                        description: A mapping of property names and values, used
                          to configure Hive. Properties that conflict with values
                          set by the Cloud Dataproc API may be overwritten. Can include
                          properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml,
                          and classes in user code.
                        type: object
                      queryFileURI:
                        description: HCFS URI of file containing Hive script to execute
                          as the job. Conflicts with query_list
                        type: string
                      queryList:
                        description: The list of Hive queries or statements to execute
                          as part of the job. Conflicts with query_file_uri
                        items:
                          type: string
                        type: array
                      scriptVariables:
                        additionalProperties:
                          type: string
                        description: 'Mapping of query variable names to values (equivalent
                          to the Hive command: SET name="value";).'
                        type: object
                    type: object
                  id:
                    type: string
                  labels:
                    additionalProperties:
                      type: string
                    description: Optional. The labels to associate with this job.
                    type: object
                  pigConfig:
                    description: The config of pag job.
                    properties:
                      continueOnFailure:
                        description: Whether to continue executing queries if a query
                          fails. The default value is false. Setting to true can be
                          useful when executing independent parallel queries. Defaults
                          to false.
                        type: boolean
                      jarFileUris:
                        description: HCFS URIs of jar files to add to the CLASSPATH
                          of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain
                          Pig UDFs.
                        items:
                          type: string
                        type: array
                      loggingConfig:
                        description: The runtime logging config of the job
                        properties:
                          driverLogLevels:
                            additionalProperties:
                              type: string
                            description: 'Optional. The per-package log levels for
                              the driver. This may include ''root'' package name to
                              configure rootLogger. Examples: ''com.google = FATAL'',
                              ''root = INFO'', ''org.apache = DEBUG''.'
                            type: object
                        required:
                        - driverLogLevels
                        type: object
                      properties:
                        additionalProperties:
                          type: string
                        description: A mapping of property names to values, used to
                          configure Pig. Properties that conflict with values set
                          by the Cloud Dataproc API may be overwritten. Can include
                          properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties,
                          and classes in user code.
                        type: object
                      queryFileURI:
                        description: HCFS URI of file containing Hive script to execute
                          as the job. Conflicts with query_list
                        type: string
                      queryList:
                        description: The list of Hive queries or statements to execute
                          as part of the job. Conflicts with query_file_uri
                        items:
                          type: string
                        type: array
                      scriptVariables:
                        additionalProperties:
                          type: string
                        description: 'Mapping of query variable names to values (equivalent
                          to the Pig command: name=[value]).'
                        type: object
                    type: object
                  placement:
                    description: The config of job placement.
                    properties:
                      clusterName:
                        description: The name of the cluster where the job will be
                          submitted
                        type: string
                      clusterUUID:
                        description: Output-only. A cluster UUID generated by the
                          Cloud Dataproc service when the job is submitted
                        type: string
                    required:
                    - clusterName
                    type: object
                  project:
                    description: The project in which the cluster can be found and
                      jobs subsequently run against. If it is not provided, the provider
                      project is used.
                    type: string
                  pysparkConfig:
                    description: The config of pySpark job.
                    properties:
                      archiveUris:
                        description: Optional. HCFS URIs of archives to be extracted
                          in the working directory of .jar, .tar, .tar.gz, .tgz, and
                          .zip
                        items:
                          type: string
                        type: array
                      args:
                        description: Optional. The arguments to pass to the driver.
                          Do not include arguments, such as --conf, that can be set
                          as job properties, since a collision may occur that causes
                          an incorrect job submission
                        items:
                          type: string
                        type: array
                      fileUris:
                        description: Optional. HCFS URIs of files to be copied to
                          the working directory of Python drivers and distributed
                          tasks. Useful for naively parallel tasks
                        items:
                          type: string
                        type: array
                      jarFileUris:
                        description: Optional. HCFS URIs of jar files to add to the
                          CLASSPATHs of the Python driver and tasks
                        items:
                          type: string
                        type: array
                      loggingConfig:
                        description: The runtime logging config of the job
                        properties:
                          driverLogLevels:
                            additionalProperties:
                              type: string
                            description: 'Optional. The per-package log levels for
                              the driver. This may include ''root'' package name to
                              configure rootLogger. Examples: ''com.google = FATAL'',
                              ''root = INFO'', ''org.apache = DEBUG''.'
                            type: object
                        required:
                        - driverLogLevels
                        type: object
                      mainPythonFileURI:
                        description: Required. The HCFS URI of the main Python file
                          to use as the driver. Must be a .py file
                        type: string
                      properties:
                        additionalProperties:
                          type: string
                        description: Optional. A mapping of property names to values,
                          used to configure PySpark. Properties that conflict with
                          values set by the Cloud Dataproc API may be overwritten.
                          Can include properties set in /etc/spark/conf/spark-defaults.conf
                          and classes in user code
                        type: object
                      pythonFileUris:
                        description: 'Optional. HCFS file URIs of Python files to
                          pass to the PySpark framework. Supported file types: .py,
                          .egg, and .zip'
                        items:
                          type: string
                        type: array
                    required:
                    - mainPythonFileURI
                    type: object
                  reference:
                    description: The reference of the job
                    properties:
                      jobID:
                        description: The job ID, which must be unique within the project.
                          The job ID is generated by the server upon job submission
                          or provided by the user as a means to perform retries without
                          creating duplicate jobs
                        type: string
                    type: object
                  region:
                    description: The Cloud Dataproc region. This essentially determines
                      which clusters are available for this job to be submitted to.
                      If not specified, defaults to global.
                    type: string
                  scheduling:
                    description: Optional. Job scheduling configuration.
                    properties:
                      maxFailuresPerHour:
                        description: Maximum number of times per hour a driver may
                          be restarted as a result of driver exiting with non-zero
                          code before job is reported failed.
                        format: int64
                        type: integer
                      maxFailuresTotal:
                        description: Maximum number of times in total a driver may
                          be restarted as a result of driver exiting with non-zero
                          code before job is reported failed.
                        format: int64
                        type: integer
                    required:
                    - maxFailuresPerHour
                    - maxFailuresTotal
                    type: object
                  sparkConfig:
                    description: The config of the Spark job.
                    properties:
                      archiveUris:
                        description: HCFS URIs of archives to be extracted in the
                          working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
                        items:
                          type: string
                        type: array
                      args:
                        description: The arguments to pass to the driver.
                        items:
                          type: string
                        type: array
                      fileUris:
                        description: HCFS URIs of files to be copied to the working
                          directory of Spark drivers and distributed tasks. Useful
                          for naively parallel tasks.
                        items:
                          type: string
                        type: array
                      jarFileUris:
                        description: HCFS URIs of jar files to add to the CLASSPATHs
                          of the Spark driver and tasks.
                        items:
                          type: string
                        type: array
                      loggingConfig:
                        description: The runtime logging config of the job
                        properties:
                          driverLogLevels:
                            additionalProperties:
                              type: string
                            description: 'Optional. The per-package log levels for
                              the driver. This may include ''root'' package name to
                              configure rootLogger. Examples: ''com.google = FATAL'',
                              ''root = INFO'', ''org.apache = DEBUG''.'
                            type: object
                        required:
                        - driverLogLevels
                        type: object
                      mainClass:
                        description: The class containing the main method of the driver.
                          Must be in a provided jar or jar that is already on the
                          classpath. Conflicts with main_jar_file_uri
                        type: string
                      mainJarFileURI:
                        description: The HCFS URI of jar file containing the driver
                          jar. Conflicts with main_class
                        type: string
                      properties:
                        additionalProperties:
                          type: string
                        description: A mapping of property names to values, used to
                          configure Spark. Properties that conflict with values set
                          by the Cloud Dataproc API may be overwritten. Can include
                          properties set in /etc/spark/conf/spark-defaults.conf and
                          classes in user code.
                        type: object
                    type: object
                  sparksqlConfig:
                    description: The config of SparkSql job
                    properties:
                      jarFileUris:
                        description: HCFS URIs of jar files to be added to the Spark
                          CLASSPATH.
                        items:
                          type: string
                        type: array
                      loggingConfig:
                        description: The runtime logging config of the job
                        properties:
                          driverLogLevels:
                            additionalProperties:
                              type: string
                            description: 'Optional. The per-package log levels for
                              the driver. This may include ''root'' package name to
                              configure rootLogger. Examples: ''com.google = FATAL'',
                              ''root = INFO'', ''org.apache = DEBUG''.'
                            type: object
                        required:
                        - driverLogLevels
                        type: object
                      properties:
                        additionalProperties:
                          type: string
                        description: A mapping of property names to values, used to
                          configure Spark SQL's SparkConf. Properties that conflict
                          with values set by the Cloud Dataproc API may be overwritten.
                        type: object
                      queryFileURI:
                        description: The HCFS URI of the script that contains SQL
                          queries. Conflicts with query_list
                        type: string
                      queryList:
                        description: The list of SQL queries or statements to execute
                          as part of the job. Conflicts with query_file_uri
                        items:
                          type: string
                        type: array
                      scriptVariables:
                        additionalProperties:
                          type: string
                        description: 'Mapping of query variable names to values (equivalent
                          to the Spark SQL command: SET name="value";).'
                        type: object
                    type: object
                  status:
                    description: The status of the job.
                    items:
                      properties:
                        details:
                          description: Output-only. Optional job state details, such
                            as an error description if the state is ERROR
                          type: string
                        state:
                          description: Output-only. A state message specifying the
                            overall job state
                          type: string
                        stateStartTime:
                          description: Output-only. The time when this state was entered
                          type: string
                        substate:
                          description: Output-only. Additional state information,
                            which includes status reported by the agent
                          type: string
                      type: object
                    type: array
                  timeouts:
                    properties:
                      create:
                        description: A Duration represents the elapsed time between
                          two instants as an int64 nanosecond count. The representation
                          limits the largest representable duration to approximately
                          290 years.
                        format: int64
                        type: integer
                      default:
                        description: A Duration represents the elapsed time between
                          two instants as an int64 nanosecond count. The representation
                          limits the largest representable duration to approximately
                          290 years.
                        format: int64
                        type: integer
                      delete:
                        description: A Duration represents the elapsed time between
                          two instants as an int64 nanosecond count. The representation
                          limits the largest representable duration to approximately
                          290 years.
                        format: int64
                        type: integer
                      read:
                        description: A Duration represents the elapsed time between
                          two instants as an int64 nanosecond count. The representation
                          limits the largest representable duration to approximately
                          290 years.
                        format: int64
                        type: integer
                      update:
                        description: A Duration represents the elapsed time between
                          two instants as an int64 nanosecond count. The representation
                          limits the largest representable duration to approximately
                          290 years.
                        format: int64
                        type: integer
                    type: object
                required:
                - placement
                type: object
              providerRef:
                description: LocalObjectReference contains enough information to let
                  you locate the referenced object inside the same namespace.
                properties:
                  name:
                    description: 'Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
                      TODO: Add other useful fields. apiVersion, kind, uid?'
                    type: string
                type: object
              resource:
                properties:
                  driverControlsFilesURI:
                    description: Output-only. If present, the location of miscellaneous
                      control files which may be used as part of job setup and handling.
                      If not present, control files may be placed in the same location
                      as driver_output_uri.
                    type: string
                  driverOutputResourceURI:
                    description: Output-only. A URI pointing to the location of the
                      stdout of the job's driver program
                    type: string
                  forceDelete:
                    description: By default, you can only delete inactive jobs within
                      Dataproc. Setting this to true, and calling destroy, will ensure
                      that the job is first cancelled before issuing the delete.
                    type: boolean
                  hadoopConfig:
                    description: The config of Hadoop job
                    properties:
                      archiveUris:
                        description: HCFS URIs of archives to be extracted in the
                          working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
                        items:
                          type: string
                        type: array
                      args:
                        description: The arguments to pass to the driver.
                        items:
                          type: string
                        type: array
                      fileUris:
                        description: HCFS URIs of files to be copied to the working
                          directory of Spark drivers and distributed tasks. Useful
                          for naively parallel tasks.
                        items:
                          type: string
                        type: array
                      jarFileUris:
                        description: HCFS URIs of jar files to add to the CLASSPATHs
                          of the Spark driver and tasks.
                        items:
                          type: string
                        type: array
                      loggingConfig:
                        description: The runtime logging config of the job
                        properties:
                          driverLogLevels:
                            additionalProperties:
                              type: string
                            description: 'Optional. The per-package log levels for
                              the driver. This may include ''root'' package name to
                              configure rootLogger. Examples: ''com.google = FATAL'',
                              ''root = INFO'', ''org.apache = DEBUG''.'
                            type: object
                        required:
                        - driverLogLevels
                        type: object
                      mainClass:
                        description: The class containing the main method of the driver.
                          Must be in a provided jar or jar that is already on the
                          classpath. Conflicts with main_jar_file_uri
                        type: string
                      mainJarFileURI:
                        description: The HCFS URI of jar file containing the driver
                          jar. Conflicts with main_class
                        type: string
                      properties:
                        additionalProperties:
                          type: string
                        description: A mapping of property names to values, used to
                          configure Spark. Properties that conflict with values set
                          by the Cloud Dataproc API may be overwritten. Can include
                          properties set in /etc/spark/conf/spark-defaults.conf and
                          classes in user code.
                        type: object
                    type: object
                  hiveConfig:
                    description: The config of hive job
                    properties:
                      continueOnFailure:
                        description: Whether to continue executing queries if a query
                          fails. The default value is false. Setting to true can be
                          useful when executing independent parallel queries. Defaults
                          to false.
                        type: boolean
                      jarFileUris:
                        description: HCFS URIs of jar files to add to the CLASSPATH
                          of the Hive server and Hadoop MapReduce (MR) tasks. Can
                          contain Hive SerDes and UDFs.
                        items:
                          type: string
                        type: array
                      properties:
                        additionalProperties:
                          type: string
                        description: A mapping of property names and values, used
                          to configure Hive. Properties that conflict with values
                          set by the Cloud Dataproc API may be overwritten. Can include
                          properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml,
                          and classes in user code.
                        type: object
                      queryFileURI:
                        description: HCFS URI of file containing Hive script to execute
                          as the job. Conflicts with query_list
                        type: string
                      queryList:
                        description: The list of Hive queries or statements to execute
                          as part of the job. Conflicts with query_file_uri
                        items:
                          type: string
                        type: array
                      scriptVariables:
                        additionalProperties:
                          type: string
                        description: 'Mapping of query variable names to values (equivalent
                          to the Hive command: SET name="value";).'
                        type: object
                    type: object
                  id:
                    type: string
                  labels:
                    additionalProperties:
                      type: string
                    description: Optional. The labels to associate with this job.
                    type: object
                  pigConfig:
                    description: The config of pag job.
                    properties:
                      continueOnFailure:
                        description: Whether to continue executing queries if a query
                          fails. The default value is false. Setting to true can be
                          useful when executing independent parallel queries. Defaults
                          to false.
                        type: boolean
                      jarFileUris:
                        description: HCFS URIs of jar files to add to the CLASSPATH
                          of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain
                          Pig UDFs.
                        items:
                          type: string
                        type: array
                      loggingConfig:
                        description: The runtime logging config of the job
                        properties:
                          driverLogLevels:
                            additionalProperties:
                              type: string
                            description: 'Optional. The per-package log levels for
                              the driver. This may include ''root'' package name to
                              configure rootLogger. Examples: ''com.google = FATAL'',
                              ''root = INFO'', ''org.apache = DEBUG''.'
                            type: object
                        required:
                        - driverLogLevels
                        type: object
                      properties:
                        additionalProperties:
                          type: string
                        description: A mapping of property names to values, used to
                          configure Pig. Properties that conflict with values set
                          by the Cloud Dataproc API may be overwritten. Can include
                          properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties,
                          and classes in user code.
                        type: object
                      queryFileURI:
                        description: HCFS URI of file containing Hive script to execute
                          as the job. Conflicts with query_list
                        type: string
                      queryList:
                        description: The list of Hive queries or statements to execute
                          as part of the job. Conflicts with query_file_uri
                        items:
                          type: string
                        type: array
                      scriptVariables:
                        additionalProperties:
                          type: string
                        description: 'Mapping of query variable names to values (equivalent
                          to the Pig command: name=[value]).'
                        type: object
                    type: object
                  placement:
                    description: The config of job placement.
                    properties:
                      clusterName:
                        description: The name of the cluster where the job will be
                          submitted
                        type: string
                      clusterUUID:
                        description: Output-only. A cluster UUID generated by the
                          Cloud Dataproc service when the job is submitted
                        type: string
                    required:
                    - clusterName
                    type: object
                  project:
                    description: The project in which the cluster can be found and
                      jobs subsequently run against. If it is not provided, the provider
                      project is used.
                    type: string
                  pysparkConfig:
                    description: The config of pySpark job.
                    properties:
                      archiveUris:
                        description: Optional. HCFS URIs of archives to be extracted
                          in the working directory of .jar, .tar, .tar.gz, .tgz, and
                          .zip
                        items:
                          type: string
                        type: array
                      args:
                        description: Optional. The arguments to pass to the driver.
                          Do not include arguments, such as --conf, that can be set
                          as job properties, since a collision may occur that causes
                          an incorrect job submission
                        items:
                          type: string
                        type: array
                      fileUris:
                        description: Optional. HCFS URIs of files to be copied to
                          the working directory of Python drivers and distributed
                          tasks. Useful for naively parallel tasks
                        items:
                          type: string
                        type: array
                      jarFileUris:
                        description: Optional. HCFS URIs of jar files to add to the
                          CLASSPATHs of the Python driver and tasks
                        items:
                          type: string
                        type: array
                      loggingConfig:
                        description: The runtime logging config of the job
                        properties:
                          driverLogLevels:
                            additionalProperties:
                              type: string
                            description: 'Optional. The per-package log levels for
                              the driver. This may include ''root'' package name to
                              configure rootLogger. Examples: ''com.google = FATAL'',
                              ''root = INFO'', ''org.apache = DEBUG''.'
                            type: object
                        required:
                        - driverLogLevels
                        type: object
                      mainPythonFileURI:
                        description: Required. The HCFS URI of the main Python file
                          to use as the driver. Must be a .py file
                        type: string
                      properties:
                        additionalProperties:
                          type: string
                        description: Optional. A mapping of property names to values,
                          used to configure PySpark. Properties that conflict with
                          values set by the Cloud Dataproc API may be overwritten.
                          Can include properties set in /etc/spark/conf/spark-defaults.conf
                          and classes in user code
                        type: object
                      pythonFileUris:
                        description: 'Optional. HCFS file URIs of Python files to
                          pass to the PySpark framework. Supported file types: .py,
                          .egg, and .zip'
                        items:
                          type: string
                        type: array
                    required:
                    - mainPythonFileURI
                    type: object
                  reference:
                    description: The reference of the job
                    properties:
                      jobID:
                        description: The job ID, which must be unique within the project.
                          The job ID is generated by the server upon job submission
                          or provided by the user as a means to perform retries without
                          creating duplicate jobs
                        type: string
                    type: object
                  region:
                    description: The Cloud Dataproc region. This essentially determines
                      which clusters are available for this job to be submitted to.
                      If not specified, defaults to global.
                    type: string
                  scheduling:
                    description: Optional. Job scheduling configuration.
                    properties:
                      maxFailuresPerHour:
                        description: Maximum number of times per hour a driver may
                          be restarted as a result of driver exiting with non-zero
                          code before job is reported failed.
                        format: int64
                        type: integer
                      maxFailuresTotal:
                        description: Maximum number of times in total a driver may
                          be restarted as a result of driver exiting with non-zero
                          code before job is reported failed.
                        format: int64
                        type: integer
                    required:
                    - maxFailuresPerHour
                    - maxFailuresTotal
                    type: object
                  sparkConfig:
                    description: The config of the Spark job.
                    properties:
                      archiveUris:
                        description: HCFS URIs of archives to be extracted in the
                          working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
                        items:
                          type: string
                        type: array
                      args:
                        description: The arguments to pass to the driver.
                        items:
                          type: string
                        type: array
                      fileUris:
                        description: HCFS URIs of files to be copied to the working
                          directory of Spark drivers and distributed tasks. Useful
                          for naively parallel tasks.
                        items:
                          type: string
                        type: array
                      jarFileUris:
                        description: HCFS URIs of jar files to add to the CLASSPATHs
                          of the Spark driver and tasks.
                        items:
                          type: string
                        type: array
                      loggingConfig:
                        description: The runtime logging config of the job
                        properties:
                          driverLogLevels:
                            additionalProperties:
                              type: string
                            description: 'Optional. The per-package log levels for
                              the driver. This may include ''root'' package name to
                              configure rootLogger. Examples: ''com.google = FATAL'',
                              ''root = INFO'', ''org.apache = DEBUG''.'
                            type: object
                        required:
                        - driverLogLevels
                        type: object
                      mainClass:
                        description: The class containing the main method of the driver.
                          Must be in a provided jar or jar that is already on the
                          classpath. Conflicts with main_jar_file_uri
                        type: string
                      mainJarFileURI:
                        description: The HCFS URI of jar file containing the driver
                          jar. Conflicts with main_class
                        type: string
                      properties:
                        additionalProperties:
                          type: string
                        description: A mapping of property names to values, used to
                          configure Spark. Properties that conflict with values set
                          by the Cloud Dataproc API may be overwritten. Can include
                          properties set in /etc/spark/conf/spark-defaults.conf and
                          classes in user code.
                        type: object
                    type: object
                  sparksqlConfig:
                    description: The config of SparkSql job
                    properties:
                      jarFileUris:
                        description: HCFS URIs of jar files to be added to the Spark
                          CLASSPATH.
                        items:
                          type: string
                        type: array
                      loggingConfig:
                        description: The runtime logging config of the job
                        properties:
                          driverLogLevels:
                            additionalProperties:
                              type: string
                            description: 'Optional. The per-package log levels for
                              the driver. This may include ''root'' package name to
                              configure rootLogger. Examples: ''com.google = FATAL'',
                              ''root = INFO'', ''org.apache = DEBUG''.'
                            type: object
                        required:
                        - driverLogLevels
                        type: object
                      properties:
                        additionalProperties:
                          type: string
                        description: A mapping of property names to values, used to
                          configure Spark SQL's SparkConf. Properties that conflict
                          with values set by the Cloud Dataproc API may be overwritten.
                        type: object
                      queryFileURI:
                        description: The HCFS URI of the script that contains SQL
                          queries. Conflicts with query_list
                        type: string
                      queryList:
                        description: The list of SQL queries or statements to execute
                          as part of the job. Conflicts with query_file_uri
                        items:
                          type: string
                        type: array
                      scriptVariables:
                        additionalProperties:
                          type: string
                        description: 'Mapping of query variable names to values (equivalent
                          to the Spark SQL command: SET name="value";).'
                        type: object
                    type: object
                  status:
                    description: The status of the job.
                    items:
                      properties:
                        details:
                          description: Output-only. Optional job state details, such
                            as an error description if the state is ERROR
                          type: string
                        state:
                          description: Output-only. A state message specifying the
                            overall job state
                          type: string
                        stateStartTime:
                          description: Output-only. The time when this state was entered
                          type: string
                        substate:
                          description: Output-only. Additional state information,
                            which includes status reported by the agent
                          type: string
                      type: object
                    type: array
                  timeouts:
                    properties:
                      create:
                        description: A Duration represents the elapsed time between
                          two instants as an int64 nanosecond count. The representation
                          limits the largest representable duration to approximately
                          290 years.
                        format: int64
                        type: integer
                      default:
                        description: A Duration represents the elapsed time between
                          two instants as an int64 nanosecond count. The representation
                          limits the largest representable duration to approximately
                          290 years.
                        format: int64
                        type: integer
                      delete:
                        description: A Duration represents the elapsed time between
                          two instants as an int64 nanosecond count. The representation
                          limits the largest representable duration to approximately
                          290 years.
                        format: int64
                        type: integer
                      read:
                        description: A Duration represents the elapsed time between
                          two instants as an int64 nanosecond count. The representation
                          limits the largest representable duration to approximately
                          290 years.
                        format: int64
                        type: integer
                      update:
                        description: A Duration represents the elapsed time between
                          two instants as an int64 nanosecond count. The representation
                          limits the largest representable duration to approximately
                          290 years.
                        format: int64
                        type: integer
                    type: object
                required:
                - placement
                type: object
              terminationPolicy:
                enum:
                - Delete
                - DoNotTerminate
                type: string
              updatePolicy:
                enum:
                - Destroy
                - DoNotDestroy
                type: string
            required:
            - providerRef
            - resource
            type: object
          status:
            properties:
              conditions:
                items:
                  properties:
                    lastTransitionTime:
                      description: Last time the condition transitioned from one status
                        to another. This should be when the underlying condition changed.  If
                        that is not known, then using the time when the API field
                        changed is acceptable.
                      format: date-time
                      type: string
                    message:
                      description: A human readable message indicating details about
                        the transition. This field may be empty.
                      type: string
                    observedGeneration:
                      description: If set, this represents the .metadata.generation
                        that the condition was set based upon. For instance, if .metadata.generation
                        is currently 12, but the .status.condition[x].observedGeneration
                        is 9, the condition is out of date with respect to the current
                        state of the instance.
                      format: int64
                      type: integer
                    reason:
                      description: The reason for the condition's last transition
                        in CamelCase. The specific API may choose whether or not this
                        field is considered a guaranteed API. This field may not be
                        empty.
                      type: string
                    status:
                      description: Status of the condition, one of True, False, Unknown.
                      type: string
                    type:
                      description: Type of condition in CamelCase or in foo.example.com/CamelCase.
                        Many .condition.type values are consistent across resources
                        like Available, but because arbitrary conditions can be useful
                        (see .node.status.conditions), the ability to deconflict is
                        important.
                      type: string
                  required:
                  - lastTransitionTime
                  - message
                  - reason
                  - status
                  - type
                  type: object
                type: array
              observedGeneration:
                description: Resource generation, which is updated on mutation by
                  the API Server.
                format: int64
                type: integer
              phase:
                description: Status defines the set of statuses a resource can have.
                type: string
            type: object
        type: object
    served: true
    storage: true
    subresources:
      status: {}
